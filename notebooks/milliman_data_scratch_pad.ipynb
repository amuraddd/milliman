{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426b6060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers\n",
    "# !pip install git+https://github.com/huggingface/transformers.git\n",
    "# !pip install xgboost\n",
    "# !pipreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55704d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(os.listdir())\n",
    "from utils.utils import notebook_line_magic\n",
    "notebook_line_magic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c27033",
   "metadata": {},
   "source": [
    "### Data Exploration \n",
    "- The nlp dataset has vector of inconsistent lenghts.\n",
    "- Couple of options (among many):\n",
    "- Since each integer represents a token. Each vector can be padded with an unused integer to make all vector have a consistent lenght -> then do PCA if needed to reduce dimensions.\n",
    "- Use a pretrained encoder to generate embeddings.\n",
    "\n",
    "\n",
    "## data processing\n",
    "- manually add padding and use sentence transformer to generate embeddings\n",
    "- use auto tokenize with padding\n",
    "- manually add padding and then do tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6d590e",
   "metadata": {},
   "source": [
    "## modeling ideas\n",
    "- create a dataloader with a defined bacth size\n",
    "- use the dataloader to create embeddings and then merge with the UniqueID\n",
    "- combined embeddings with other features\n",
    "- build an XGBoost model and a custom torch fully connected network\n",
    "- you can als get fancy with concatenating the bert model above with a torch classifier \n",
    "- knn classifier might also work well with features and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd60ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding to the tokens\n",
    "import ast\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "from models.pca import get_pca_components\n",
    "from models.xgboost_train_predict import search_best_params\n",
    "from utils.data_utils import (\n",
    "    get_data_df, get_padded_tokens, generate_embeddings_from_tokens\n",
    ")\n",
    "from utils.utils import (\n",
    "    plot_class_distribution, plot_pca_components_and_variance,\n",
    "    generate_2d_pca_distribution_plot, generate_3d_pca_distribution_plot\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a23fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "data_type = 'train'\n",
    "data_path = Path(data_path)\n",
    "embeddings_file = f'x_{data_type}_nlp_embeddings.csv'\n",
    "embeddings_file_path = data_path/data_type/embeddings_file\n",
    "\n",
    "# combine embeddings with other features for modeling\n",
    "embeddings_df = pd.read_csv(embeddings_file_path).drop(columns=['Unnamed: 0'])\n",
    "data_df = get_data_df(data_type='train')\n",
    "data_df = data_df.merge(embeddings_df, on='UniqueID', how='inner')\n",
    "data_df.set_index('UniqueID', inplace=True)\n",
    "# data_df.drop(columns=['nlp_feature_vector'], inplace=True)\n",
    "\n",
    "# x = data_df[data_df.columns[~data_df.columns.isin(['Target'])]]\n",
    "# y = data_df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f5208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inconsistent input size\n",
    "# temp = [np.array(eval(i)) for i in data_df.iloc[0:3]['nlp_feature_vector'].values]\n",
    "# for i in temp:\n",
    "#     print(i.shape)\n",
    "\n",
    "\n",
    "features = data_df[['Feature1', 'Feature2', 'Feature3', 'Feature4']]\n",
    "features.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e7c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PCA components\n",
    "# for c in [2,100, 200, 300, 500]:\n",
    "#     mod, z, z0 = get_pca_components(data=x, num_components=c)\n",
    "#     plot_pca_components_and_variance(mod, save_fig=True)\n",
    "# plot_class_distribution(target=y, save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad45be6",
   "metadata": {},
   "source": [
    "- based on above it seems like having more features will continue to boost performance\n",
    "- so maybe not reduce dimensionality while fitting the model\n",
    "- a deep learning model may work well compared to a tree based model but it could potentially overfit to the data too.. experiment and find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c01497",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, z, z0 = get_pca_components(data=x, num_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e29b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_2d_pca_distribution_plot(\n",
    "    components=z0,\n",
    "    target=y\n",
    ")\n",
    "generate_3d_pca_distribution_plot(\n",
    "    components=z0,\n",
    "    target=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea6b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_best_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
